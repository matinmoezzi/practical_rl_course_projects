{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of practice_reinforce.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce.ipynb","timestamp":1603829967006}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Iml9FUv8WkI4"},"source":["# REINFORCE in TensorFlow\n","\n","Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","metadata":{"id":"EbBJhmTUWkI6","executionInfo":{"status":"ok","timestamp":1603828264401,"user_tz":-210,"elapsed":1529,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"769e37d4-8810-4327-f55b-befad8b58329","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import sys, os\n","if 'google.colab' in sys.modules:\n","    %tensorflow_version 1.x\n","    \n","    if not os.path.exists('.setup_complete'):\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n","\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","        !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Starting virtual X frame buffer: Xvfb.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B5z0W5-GWkJD","executionInfo":{"status":"ok","timestamp":1603828264403,"user_tz":-210,"elapsed":1338,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o4NOgYmFWkJK"},"source":["A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."]},{"cell_type":"code","metadata":{"id":"oom1KVGAWkJM","executionInfo":{"status":"ok","timestamp":1603828265688,"user_tz":-210,"elapsed":2295,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"50dd12b1-b8bb-4c33-b34e-fc455e00558b","colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["env = gym.make(\"CartPole-v0\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape\n","\n","plt.imshow(env.render(\"rgb_array\"))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f6f92cd26a0>"]},"metadata":{"tags":[]},"execution_count":3},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS9klEQVR4nO3df6zd9X3f8efLPzBZTWrAt46xzQyNV0anxkR3xFHyByVKS9BUUymNYBNYEZI7iSyJFG2DTloTaSipMsKWrEOjgoYsWQgtibAQW0oJG8qkACYxDuDQmGCKPRtffhkowcX2e3/cr8mJueb+PBx/7nk+pKPz/b6/n+85749yeOX4c7/nnFQVkqR2LBh0A5Kk6TG4JakxBrckNcbglqTGGNyS1BiDW5Ia07fgTnJRkseT7Exydb+eR5KGTfpxHXeShcDfAB8GdgMPApdV1WNz/mSSNGT69Y77fGBnVf2sqv4euBXY2KfnkqShsqhPj7sKeLpnfzfwvuMNXr58ea1du7ZPrUhSe3bt2sWzzz6biY71K7gnlWQzsBngzDPPZOvWrYNqRZJOOKOjo8c91q+lkj3Amp791V3tDVV1Y1WNVtXoyMhIn9qQpPmnX8H9ILAuyVlJTgIuBbb06bkkaaj0Zamkqg4l+QTwXWAhcHNVPdqP55KkYdO3Ne6qugu4q1+PL0nDyk9OSlJjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqzKx+uizJLuBl4DBwqKpGk5wGfAtYC+wCPlZVL8yuTUnSUXPxjvu3q2p9VY12+1cD91TVOuCebl+SNEf6sVSyEbil274FuKQPzyFJQ2u2wV3AXyV5KMnmrraiqvZ22/uAFbN8DklSj1mtcQMfrKo9SX4NuDvJT3oPVlUlqYlO7IJ+M8CZZ545yzYkaXjM6h13Ve3p7vcD3wHOB55JshKgu99/nHNvrKrRqhodGRmZTRuSNFRmHNxJfiXJKUe3gd8BHgG2AJu6YZuAO2bbpCTpF2azVLIC+E6So4/zP6rqfyV5ELgtyZXAU8DHZt+mJOmoGQd3Vf0MeM8E9eeAD82mKUnS8fnJSUlqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxkwZ3kpuT7E/ySE/ttCR3J/lpd39qV0+SLyfZmWR7kvf2s3lJGkZTecf9VeCiY2pXA/dU1Trgnm4f4CPAuu62GbhhbtqUJB01aXBX1X3A88eUNwK3dNu3AJf01L9W434ALEuycq6alSTNfI17RVXt7bb3ASu67VXA0z3jdne1N0myOcnWJFvHxsZm2IYkDZ9Z/3GyqgqoGZx3Y1WNVtXoyMjIbNuQpKEx0+B+5ugSSHe/v6vvAdb0jFvd1SRJc2Smwb0F2NRtbwLu6Klf0V1dsgE40LOkIkmaA4smG5Dkm8AFwPIku4E/Br4A3JbkSuAp4GPd8LuAi4GdwKvAx/vQsyQNtUmDu6ouO86hD00wtoCrZtuUJOn4/OSkJDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGTBrcSW5Osj/JIz21zybZk2Rbd7u459g1SXYmeTzJ7/arcUkaVlN5x/1V4KIJ6tdX1frudhdAknOBS4Hf7M75r0kWzlWzkqQpBHdV3Qc8P8XH2wjcWlUHq+pJxn/t/fxZ9CdJOsZs1rg/kWR7t5RyaldbBTzdM2Z3V3uTJJuTbE2ydWxsbBZtSNJwmWlw3wD8OrAe2AtcN90HqKobq2q0qkZHRkZm2IYkDZ8ZBXdVPVNVh6vqCPBn/GI5ZA+wpmfo6q4mSZojMwruJCt7dn8fOHrFyRbg0iRLkpwFrAMemF2LkqReiyYbkOSbwAXA8iS7gT8GLkiyHihgF/CHAFX1aJLbgMeAQ8BVVXW4P61L0nCaNLir6rIJyje9xfhrgWtn05Qk6fj85KQkNcbglqTGGNyS1BiDW5IaY3BLUmMmvapEGgavPrebQ6+9wpJTlrPkncsH3Y70lgxuCfh/W7dw4KmHWfLOEZa8c/wrGE4+9QzWvP8PBtyZ9GYGt9Tj4EtjHHxp/EvPDr9+cMDdSBNzjVuSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYyYN7iRrktyb5LEkjyb5VFc/LcndSX7a3Z/a1ZPky0l2Jtme5L39noQkDZOpvOM+BHymqs4FNgBXJTkXuBq4p6rWAfd0+wAfYfzX3dcBm4Eb5rxrSRpikwZ3Ve2tqh922y8DO4BVwEbglm7YLcAl3fZG4Gs17gfAsiQr57xzSRpS01rjTrIWOA+4H1hRVXu7Q/uAFd32KuDpntN2d7VjH2tzkq1Jto6NjU2zbUkaXlMO7iRLgduBT1fVS73HqqqAms4TV9WNVTVaVaMjIyPTOVWShtqUgjvJYsZD+xtV9e2u/MzRJZDufn9X3wOs6Tl9dVeTJM2BqVxVEuAmYEdVfann0BZgU7e9Cbijp35Fd3XJBuBAz5KKJGmWpvILOB8ALgd+nGRbV/sj4AvAbUmuBJ4CPtYduwu4GNgJvAp8fE47lqQhN2lwV9X3gRzn8IcmGF/AVbPsS5J0HH5yUpIaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSY6byY8Frktyb5LEkjyb5VFf/bJI9SbZ1t4t7zrkmyc4kjyf53X5OQJKGzVR+LPgQ8Jmq+mGSU4CHktzdHbu+qv5j7+Ak5wKXAr8JnAH8dZJ/VFWH57JxSRpWk77jrqq9VfXDbvtlYAew6i1O2QjcWlUHq+pJxn/t/fy5aFbqh0MH/45DP3/pTfWTl71rAN1Ik5vWGneStcB5wP1d6RNJtie5OcmpXW0V8HTPabt566CXBuq1F/bxd/uffFN9+W98YADdSJObcnAnWQrcDny6ql4CbgB+HVgP7AWum84TJ9mcZGuSrWNjY9M5VZKG2pSCO8lixkP7G1X1bYCqeqaqDlfVEeDP+MVyyB5gTc/pq7vaL6mqG6tqtKpGR0ZGZjMHSRoqU7mqJMBNwI6q+lJPfWXPsN8HHum2twCXJlmS5CxgHfDA3LUsScNtKleVfAC4HPhxkm1d7Y+Ay5KsBwrYBfwhQFU9muQ24DHGr0i5yitKJGnuTBrcVfV9IBMcuustzrkWuHYWfUmSjsNPTkpSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDVmKl/rKjVnz549fPKTn+TIkSOTjl29bDGbNpxKjvkOzGuuuYbdL74+6fkLFizgK1/5CmecccZM25WmxeDWvPTKK69wxx13cPjw5F8F/1tnr+CKDZfw94dPfqO2eMFr3HfffWz/2TOTnr9w4UI+//nPz6pfaToMbgn421f/MTtePp/xr54vzjnlQYo7Bt2WNCHXuDX0Dh55B3/76jkcrpM4XIs5XCex46X3ceD15YNuTZqQwa2h9+rhd3Lg9dN/qXaERZT/eegENZUfCz45yQNJHk7yaJLPdfWzktyfZGeSbyU5qasv6fZ3dsfX9ncK0uycsuh5Tj9p3y/VFuc1FmbyP0xKgzCVtxQHgQur6j3AeuCiJBuAPwGur6p3Ay8AV3bjrwRe6OrXd+OkE9bhQz9n4Sv/h2effZLXf76PpYteYP2y/807Fz0/6NakCU3lx4ILeKXbXdzdCrgQ+Odd/Rbgs8ANwMZuG+Avgf+SJN3jSCecx59+jn/1hesowjlnLuc3zjyd/wvsHntp0K1JE5rSVSVJFgIPAe8G/hR4Anixqg51Q3YDq7rtVcDTAFV1KMkB4HTg2eM9/r59+/jiF784owlIExkbG5vSNdxHHakCih1P7WfHU/un9VxHjhzhpptuYvly/5ipubNv377jHptScFfVYWB9kmXAd4BzZttUks3AZoBVq1Zx+eWXz/YhpTc88cQTXHfddbwd/9BbsGABGzdu5Oyzz+77c2l4fP3rXz/usWldx11VLya5F3g/sCzJou5d92pgTzdsD7AG2J1kEfCrwHMTPNaNwI0Ao6Oj9a53vWs6rUhv6cCBA+TYj0L20fLly/E1rLm0ePHi4x6bylUlI907bZK8A/gwsAO4F/hoN2wTvPFphS3dPt3x77m+LUlzZyrvuFcCt3Tr3AuA26rqziSPAbcm+Q/Aj4CbuvE3Af89yU7geeDSPvQtSUNrKleVbAfOm6D+M+D8CeqvAX8wJ91Jkt7Ej4ZJUmMMbklqjN8OqHlp6dKlbNy4cVrXcs/UggULWLp0ad+fRzrK4Na8tGrVKm6//fZBtyH1hUslktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxU/mx4JOTPJDk4SSPJvlcV/9qkieTbOtu67t6knw5yc4k25O8t9+TkKRhMpXv4z4IXFhVryRZDHw/yf/sjv3rqvrLY8Z/BFjX3d4H3NDdS5LmwKTvuGvcK93u4u5Wb3HKRuBr3Xk/AJYlWTn7ViVJMMU17iQLk2wD9gN3V9X93aFru+WQ65Ms6WqrgKd7Tt/d1SRJc2BKwV1Vh6tqPbAaOD/JPwGuAc4B/ilwGvBvp/PESTYn2Zpk69jY2DTblqThNa2rSqrqReBe4KKq2tsthxwE/hw4vxu2B1jTc9rqrnbsY91YVaNVNToyMjKz7iVpCE3lqpKRJMu67XcAHwZ+cnTdOkmAS4BHulO2AFd0V5dsAA5U1d6+dC9JQ2gqV5WsBG5JspDxoL+tqu5M8r0kI0CAbcC/7MbfBVwM7AReBT4+921L0vCaNLirajtw3gT1C48zvoCrZt+aJGkifnJSkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1JlU16B5I8jLw+KD76JPlwLODbqIP5uu8YP7OzXm15R9W1chEBxa93Z0cx+NVNTroJvohydb5OLf5Oi+Yv3NzXvOHSyWS1BiDW5Iac6IE942DbqCP5uvc5uu8YP7OzXnNEyfEHyclSVN3orzjliRN0cCDO8lFSR5PsjPJ1YPuZ7qS3Jxkf5JHemqnJbk7yU+7+1O7epJ8uZvr9iTvHVznby3JmiT3JnksyaNJPtXVm55bkpOTPJDk4W5en+vqZyW5v+v/W0lO6upLuv2d3fG1g+x/MkkWJvlRkju7/fkyr11JfpxkW5KtXa3p1+JsDDS4kywE/hT4CHAucFmScwfZ0wx8FbjomNrVwD1VtQ64p9uH8Xmu626bgRveph5n4hDwmao6F9gAXNX9b9P63A4CF1bVe4D1wEVJNgB/AlxfVe8GXgCu7MZfCbzQ1a/vxp3IPgXs6NmfL/MC+O2qWt9z6V/rr8WZq6qB3YD3A9/t2b8GuGaQPc1wHmuBR3r2HwdWdtsrGb9OHeC/AZdNNO5EvwF3AB+eT3MD/gHwQ+B9jH+AY1FXf+N1CXwXeH+3vagbl0H3fpz5rGY8wC4E7gQyH+bV9bgLWH5Mbd68Fqd7G/RSySrg6Z793V2tdSuqam+3vQ9Y0W03Od/un9HnAfczD+bWLSdsA/YDdwNPAC9W1aFuSG/vb8yrO34AOP3t7XjK/hPwb4Aj3f7pzI95ARTwV0keSrK5qzX/WpypE+WTk/NWVVWSZi/dSbIUuB34dFW9lOSNY63OraoOA+uTLAO+A5wz4JZmLck/A/ZX1UNJLhh0P33wwarak+TXgLuT/KT3YKuvxZka9DvuPcCanv3VXa11zyRZCdDd7+/qTc03yWLGQ/sbVfXtrjwv5gZQVS8C9zK+hLAsydE3Mr29vzGv7vivAs+9za1OxQeA30uyC7iV8eWS/0z78wKgqvZ09/sZ/z/b85lHr8XpGnRwPwis6/7yfRJwKbBlwD3NhS3Apm57E+Prw0frV3R/9d4AHOj5p94JJeNvrW8CdlTVl3oONT23JCPdO22SvIPxdfsdjAf4R7thx87r6Hw/CnyvuoXTE0lVXVNVq6tqLeP/HX2vqv4Fjc8LIMmvJDnl6DbwO8AjNP5anJVBL7IDFwN/w/g6478bdD8z6P+bwF7gdcbX0q5kfK3wHuCnwF8Dp3Vjw/hVNE8APwZGB93/W8zrg4yvK24HtnW3i1ufG/BbwI+6eT0C/PuufjbwALAT+AtgSVc/udvf2R0/e9BzmMIcLwDunC/z6ubwcHd79GhOtP5anM3NT05KUmMGvVQiSZomg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMb8fxRwd34mtRlUAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"38iAPvCaWkJU"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"id":"2Ljnkez2WkJW"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n","\n","For numerical stability, please __do not include the softmax layer into your network architecture__.\n","We'll use softmax or log-softmax where appropriate."]},{"cell_type":"code","metadata":{"id":"Qsg95zGbWkJX","executionInfo":{"status":"ok","timestamp":1603828267414,"user_tz":-210,"elapsed":3591,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["import tensorflow as tf\n","\n","sess = tf.InteractiveSession()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjR-rUG5WkJe","executionInfo":{"status":"ok","timestamp":1603828267420,"user_tz":-210,"elapsed":3403,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# create input variables. We only need <s, a, r> for REINFORCE\n","ph_states = tf.placeholder('float32', (None,) + state_dim, name=\"states\")\n","ph_actions = tf.placeholder('int32', name=\"action_ids\")\n","ph_cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkT54s-QWkJj","executionInfo":{"status":"ok","timestamp":1603829813830,"user_tz":-210,"elapsed":1282,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, InputLayer\n","\n","#<YOUR CODE: define network graph using raw TF, Keras, or any other library you prefer>\n","model = Sequential([InputLayer(state_dim),\n","                    Dense(128, activation='relu'), \n","                    Dense(n_actions, activation='linear')])\n","\n","\n","logits = model(ph_states)\n","\n","policy = tf.nn.softmax(logits)\n","log_policy = tf.nn.log_softmax(logits)"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6t4VdLkWkJq","executionInfo":{"status":"ok","timestamp":1603829813832,"user_tz":-210,"elapsed":1062,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Initialize model parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":85,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nt_pt-uWWkJy","executionInfo":{"status":"ok","timestamp":1603829813834,"user_tz":-210,"elapsed":862,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    return policy.eval({ph_states: [states]})[0]"],"execution_count":86,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AR9RviQpWkJ5"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","metadata":{"id":"t0GTlJlaWkJ6","executionInfo":{"status":"ok","timestamp":1603829814258,"user_tz":-210,"elapsed":974,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    Play a full session with REINFORCE agent.\n","    Returns sequences of states, actions, and rewards.\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(s)\n","        \n","        # Sample action with given probabilities.\n","        a = np.random.choice(range(env.action_space.n), p=action_probs)\n","        new_s, r, done, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if done:\n","            break\n","\n","    return states, actions, rewards"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KiI1WbzWkKA","executionInfo":{"status":"ok","timestamp":1603829814261,"user_tz":-210,"elapsed":784,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# test it\n","states, actions, rewards = generate_session(env)"],"execution_count":88,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RCxlGxf8WkKF"},"source":["### Computing cumulative rewards\n","\n","$$\n","\\begin{align*}\n","G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n","&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n","&= r_t + \\gamma * G_{t + 1}\n","\\end{align*}\n","$$"]},{"cell_type":"code","metadata":{"id":"6sUfSP-AWkKG","executionInfo":{"status":"ok","timestamp":1603829814682,"user_tz":-210,"elapsed":876,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    Take a list of immediate rewards r(s,a) for the whole session \n","    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n","    \n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    A simple way to compute cumulative rewards is to iterate from the last\n","    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    G = [0] * len(rewards)\n","    G[-1] = rewards[-1]\n","    for i in range(len(rewards) - 2, -1, -1):\n","      G[i] = rewards[i] + gamma*G[i+1]\n","\n","    return G"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWd-7HFxWkKL","executionInfo":{"status":"ok","timestamp":1603829814684,"user_tz":-210,"elapsed":718,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"b110d2bd-3dee-46b0-8762-6a22f46ecc96","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["assert len(get_cumulative_rewards(range(100))) == 100\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n","    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n","    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(\n","    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n","    [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"],"execution_count":90,"outputs":[{"output_type":"stream","text":["looks good!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RTr1FDNQWkKR"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n","\n","$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n","\n","$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n","\n","When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."]},{"cell_type":"code","metadata":{"id":"GVZd0nFrWkKT","executionInfo":{"status":"ok","timestamp":1603829816082,"user_tz":-210,"elapsed":961,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# This code selects the log-probabilities (log pi(a_i|s_i)) for those actions that were actually played.\n","indices = tf.stack([tf.range(tf.shape(log_policy)[0]), ph_actions], axis=-1)\n","log_policy_for_actions = tf.gather_nd(log_policy, indices)"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVcIK31RWkKa","executionInfo":{"status":"ok","timestamp":1603829816084,"user_tz":-210,"elapsed":752,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n","# You may use log_policy_for_actions to get log probabilities for actions taken.\n","# Also recall that we defined ph_cumulative_rewards earlier.\n","\n","J = tf.reduce_mean(log_policy_for_actions * ph_cumulative_rewards, axis=-1)"],"execution_count":92,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_JCpBuRWkKg"},"source":["As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n","\n","$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"]},{"cell_type":"code","metadata":{"id":"Uk14XcHcWkKh","executionInfo":{"status":"ok","timestamp":1603829816651,"user_tz":-210,"elapsed":588,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n","# being deterministic, harming exploration.\n","\n","entropy = 0 - tf.reduce_mean(policy*log_policy)"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"id":"wxtNDFZcWkKm","executionInfo":{"status":"ok","timestamp":1603829818003,"user_tz":-210,"elapsed":1400,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# # Maximizing X is the same as minimizing -X, hence the sign.\n","loss = -(J + 0.1 * entropy)\n","\n","update = tf.train.AdamOptimizer().minimize(loss)"],"execution_count":94,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wgn8aaWyWkKs","executionInfo":{"status":"ok","timestamp":1603829818005,"user_tz":-210,"elapsed":1149,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["def train_on_session(states, actions, rewards, t_max=1000):\n","    \"\"\"given full session, trains agent with policy gradient\"\"\"\n","    cumulative_rewards = get_cumulative_rewards(rewards)\n","    update.run({\n","        ph_states: states,\n","        ph_actions: actions,\n","        ph_cumulative_rewards: cumulative_rewards,\n","    })\n","    return sum(rewards)"],"execution_count":95,"outputs":[]},{"cell_type":"code","metadata":{"id":"HolW0I69WkKx","executionInfo":{"status":"ok","timestamp":1603829818007,"user_tz":-210,"elapsed":777,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Initialize optimizer parameters\n","sess.run(tf.global_variables_initializer())"],"execution_count":96,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZuwU0dVBWkK3"},"source":["### The actual training"]},{"cell_type":"code","metadata":{"id":"v_mAYl7yWkK4","executionInfo":{"status":"ok","timestamp":1603829881565,"user_tz":-210,"elapsed":63632,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"3b1f3e34-2bdc-4067-e6be-b416e1604640","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n","\n","    print(\"mean reward: %.3f\" % (np.mean(rewards)))\n","\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"],"execution_count":97,"outputs":[{"output_type":"stream","text":["mean reward: 28.250\n","mean reward: 31.300\n","mean reward: 48.060\n","mean reward: 71.430\n","mean reward: 145.530\n","mean reward: 147.390\n","mean reward: 260.810\n","mean reward: 409.420\n","You Win!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QPNAmtWzWkK_"},"source":["### Results & video"]},{"cell_type":"code","metadata":{"id":"SLVifqV4WkLB","executionInfo":{"status":"ok","timestamp":1603829908540,"user_tz":-210,"elapsed":24694,"user":{"displayName":"","photoUrl":"","userId":""}}},"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"],"execution_count":98,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKUtgjNZWkLG","executionInfo":{"status":"ok","timestamp":1603829929995,"user_tz":-210,"elapsed":839,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"5719ce71-00af-41db-bf6b-7db54f7d9782","colab":{"base_uri":"https://localhost:8080/","height":501}},"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.1.1722.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"jDTp5_oXWkLK","executionInfo":{"status":"ok","timestamp":1603829955788,"user_tz":-210,"elapsed":23145,"user":{"displayName":"","photoUrl":"","userId":""}},"outputId":"bc82b753-5f3e-4d22-cb95-c02111cae833","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from submit import submit_cartpole\n","submit_cartpole(generate_session, 'matin.moezi@aut.ac.ir', 'BLf4JYFbbXabt5qW')"],"execution_count":100,"outputs":[{"output_type":"stream","text":["Your average reward is 421.75 over 100 episodes\n","Submitted to Coursera platform. See results on assignment page!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JMSpDj5OWkLQ"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}]}